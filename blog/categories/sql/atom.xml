<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Sql | PracticalWeb Ltd]]></title>
  <link href="http://www.practicalweb.co.uk/blog/categories/sql/atom.xml" rel="self"/>
  <link href="http://www.practicalweb.co.uk/"/>
  <updated>2014-10-31T17:58:19+00:00</updated>
  <id>http://www.practicalweb.co.uk/</id>
  <author>
    <name><![CDATA[Sean Burlington]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Drupal Live and Dev Sync]]></title>
    <link href="http://www.practicalweb.co.uk/blog/2009/07/23/drupal-live-and-dev-sync/"/>
    <updated>2009-07-23T00:00:00+01:00</updated>
    <id>http://www.practicalweb.co.uk/blog/2009/07/23/drupal-live-and-dev-sync</id>
    <content type="html"><![CDATA[<p>
When developing Drupal one often needs to pull recent copies of the live database into the dev environment.
</p>


<p>
Loading a dump into the dev database will update any existing tables, add any new ones - but it won&#8217;t remove tables from the dev environment that re not in live.
</p>


<p>
This causes problems with Drupal as module install and update hooks may need to create tables which don&#8217;t yet exist on live. 
</p>


<p>
My solution which assumes you have .my.cnf set up to provide login locally is below.
</p>


<p><code></p>

<h1>dump live db</h1>

<p>ssh live &ldquo;mysqldump livedb | gzip > /tmp/livedb.sql.gz&rdquo;</p>

<h1>download live db</h1>

<p>scp live:/tmp/livedb.sql.gz /tmp</p>

<h1>clean out dev db</h1>

<p>mysql -BNe &ldquo;show tables&rdquo; devdb | awk &lsquo;{print &ldquo;drop table &rdquo; $1 &ldquo;;&rdquo;} | mysql devdb</p>

<h1>copy live db to dev</h1>

<p>zcat /tmp/livedb.sql.gz | mysql devdb</p>

<h1>make sure test mails never go to real users!</h1>

<p>echo &ldquo;update users set mail=&lsquo;<a href="&#109;&#97;&#105;&#108;&#x74;&#x6f;&#58;&#x6d;&#101;&#64;&#101;&#x78;&#97;&#109;&#112;&#x6c;&#101;&#46;&#99;&#x6f;&#x6d;">&#109;&#x65;&#x40;&#x65;&#120;&#x61;&#x6d;&#112;&#x6c;&#x65;&#46;&#x63;&#x6f;&#109;</a>&rsquo;&rdquo; | mysql devdb
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Greatest and Least]]></title>
    <link href="http://www.practicalweb.co.uk/blog/2009/07/21/sql-greatest-and-least/"/>
    <updated>2009-07-21T00:00:00+01:00</updated>
    <id>http://www.practicalweb.co.uk/blog/2009/07/21/sql-greatest-and-least</id>
    <content type="html"><![CDATA[<p>
Most of the time I can get what I want out of SQL using the standard aggregate functions, but today I needed to find the latest timestamp from any one of three columns.
</p>


<p>
I achieved this using the GREATEST() function, it&#8217;s not party of the SQL standard but is commonly available.
</p>


<p>
GREATEST(<em>value</em> [<span class="OPTIONAL">, &#8230;</span>])<br />
</p>


<pre class="SYNOPSIS">
LEAST(<em>value</em> [<span class="OPTIONAL">, ...</span>])
</pre>


<p>
The GREATEST and LEAST
functions select the largest or smallest value from a list of any
number of expressions. 
</p>


<p><br /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drupal Schema API Has Problems]]></title>
    <link href="http://www.practicalweb.co.uk/blog/2009/06/22/drupal-schema-api-has-problems/"/>
    <updated>2009-06-22T00:00:00+01:00</updated>
    <id>http://www.practicalweb.co.uk/blog/2009/06/22/drupal-schema-api-has-problems</id>
    <content type="html"><![CDATA[<p>
<a href="http://www.lullabot.com/articles/drupocalypse-now-or-dangerous-integer-handling-drupal-write-record">An interesting post from Lullabot today</a> on problems with Drupal&#8217;s Schema API and the <a href="http://www.google.co.uk/search?q=Twitpocalypse">Twitpocalypse</a>
</p>


<p>
It turns out that <a href="http://drupal.org/node/333788">the Schema API doesn&#8217;t properly understand the difference between different types of integers</a> 
</p>


<p>I was already concerned that
<a href="http://drupal.org/node/200953">Schema API lacks the &lsquo;time&rsquo; and &lsquo;date&rsquo; type</a></p>

<p>
I don&#8217;t really understand why the Drupal team has decided to try and roll it&#8217;s own database abstraction layer, and make it a core part of the system in the current state. 
</p>


<p>
There seem to be <a href="http://drupal.org/project/issues/search/drupal?text=&amp;assigned=&amp;submitted=&amp;participant=&amp;status[]=Open&amp;priorities[]=1&amp;priorities[]=2&amp;categories[]=bug&amp;version[]=6.x&amp;component[]=database+system&amp;issue_tags=">quite a few bugs</a>
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Updating Drupal]]></title>
    <link href="http://www.practicalweb.co.uk/blog/2008/12/15/updating-drupal/"/>
    <updated>2008-12-15T00:00:00+00:00</updated>
    <id>http://www.practicalweb.co.uk/blog/2008/12/15/updating-drupal</id>
    <content type="html"><![CDATA[<p>
For most of 2008 I&#8217;ve been working on a large Drupal installation (around 10 developers,  something like 100 custom modules, 17 languages, split over 2 sites).
</p>


<p>
This all adds up to a lot of updates, and schema changes.
</p>


<p>
Drupal&#8217;s hook_install, and hook_update_N functions are pretty good, but not perfect at bringing order to this chaos.
</p>


<p>
These functions provide a basic way for developers to provide database installation and update functions, it&#8217;s great that these can combine full code functionality (eg for data migration) synchronised with schema changes. 
</p>


<p>
As we have  something like 10 Drupal installs fro various, stage, test and UAT purposes, plus 2 per developer - having these updates scripted is essential. 
</p>


<p>
But all too frequently we have issues 
</p>


<ul>
    <li>Loading a MySQL backup on top of an existing database leaves newly created  tables in place even though they don&#8217;t exist in the backup (solution - always drop and recreate the database before loading a backup).</li>
    <li>A developer changes an update hook - assuming incorrectly that it hasn&#8217;t been run yet.</li>
    <li>An update hook misses some functionality that the developer added to their own database - but forgot to put in the hook.</li>
</ul>


<p>
Another problem is that the update interface gives no warning of the changes about to be enacted - you get a list of SQL command run afterwards. 
</p>


<p>
In short I&#8217;ve found that this update system is targeted more at distribution of open source code to the public, and doesn&#8217;t work so smoothly for rapid development amongst a largish team. 
</p>


<p>
We&#8217;re still on Drupal 5 and are not likely to upgrade - the benefits just aren&#8217;t worth the costs for such a large codebase.
</p>


<p>
I&#8217;m curious though about Drupal 6&#8217;s schema API, it seems like it ought to be possible to use this to run an update that doesn&#8217;t just make changes based on version numbers. It could actually examine the existing schema, compare this to the required one - report changes it suggests and run them after they are approved.
</p>


<p>
It could also be used in development to verify that the developers altered schema is properly recorded in the code. 
</p>


<p>
I expect that level of functionality may be some way off though. 
</p>


<p>
http://drupal.org/project/schema
</p>


<p>
http://drupal.org/node/146843 
</p>


<p>
&nbsp;
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Number Crunching : Database vs OOP]]></title>
    <link href="http://www.practicalweb.co.uk/blog/2008/10/05/number-crunching-database-vs-oop/"/>
    <updated>2008-10-05T00:00:00+01:00</updated>
    <id>http://www.practicalweb.co.uk/blog/2008/10/05/number-crunching-database-vs-oop</id>
    <content type="html"><![CDATA[<p>
For the last few months I&#8217;ve been working on a project that is part website and part data processing.
</p>


<p>
In theory the website is the bulk of the project, but in practice processing the data has taken a disproportionate amount of resources.
</p>


<p>
Data exchange with another organisation is always tricky - and in this case we have incoming data that doesn&#8217;t conform to specification, business rules that change, and a daily data import to run.
</p>


<p>
The code I&#8217;ve been working on has been through a couple of versions, and now I have in mind a third 
</p>


<ol>
    <li>process data from file a line at a time, comparing each record with the database individually</li>
    <li>load all data from file into a temporary table, apply all rules as SQL updates to the whole table - joined to other tables where comparisons are needed.</li>
    <li>load all data from file into a temporary table, retrieving data joined with the comparison data - process in batches say 1000 lines at a time.</li>
</ol>


<p>
Version 1 was impossibly slow, and it was bad code we inherited so it had to go.
</p>


<p>
Version 2 is much faster, but as the business rules are documented per record and we&#8217;re performing set based operations it has proved very hard to verify that the code matches the required logic.  It&#8217;s also very hard to unit test as all the logic is performed in the database.
</p>


<p>
Version 3 probably won&#8217;t get written now - as is the way of programming - you always learn how you&#8217;d do it next time.
</p>


<p>
I have one colleague in particular who thinks that the database should do the heavy lifting as that is what it is optimised for, and to an extent I agree. But the more I get into unit testing the more I want to write code that neatly packages up logic in a testable way.
</p>


<p>
It has also become apparent just how important it is to have code that follows business rules in a clear way. It&#8217;s not enough that the output should be right. I recently handed this code to a very capable developer - he&#8217;s normally so polite - but trying to reconcile a flowchart with a long series of SQL queries tested his patience.
</p>


<p>
In the end leaning so hard on the database has made the system much harder to read, test, change, and maintain.
</p>


<p>
I wonder if version 3 would be any slower anyway - it might even be faster. In fact version 3 could be run multithreaded - or split into multiple processes&#8230;. 
</p>

]]></content>
  </entry>
  
</feed>
