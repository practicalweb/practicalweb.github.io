<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Security | PracticalWeb Ltd]]></title>
  <link href="http://www.practicalweb.co.uk/blog/categories/security/atom.xml" rel="self"/>
  <link href="http://www.practicalweb.co.uk/"/>
  <updated>2015-03-18T15:01:06+00:00</updated>
  <id>http://www.practicalweb.co.uk/</id>
  <author>
    <name><![CDATA[Sean Burlington]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Quick Steps to Secure Ubuntu Linux]]></title>
    <link href="http://www.practicalweb.co.uk/blog/2014/11/13/quick-steps-to-secure-ubuntu-linux/"/>
    <updated>2014-11-13T14:41:12+00:00</updated>
    <id>http://www.practicalweb.co.uk/blog/2014/11/13/quick-steps-to-secure-ubuntu-linux</id>
    <content type="html"><![CDATA[<p>Recently I&rsquo;ve been reviewing security and realised I&rsquo;ve been relying too much on my routers firewall - which isn&rsquo;t even present if I work on an open wifi connection somewhere.</p>

<p>Steps so far</p>

<p>Reinstalled Ubuntu with full disk encryption, apart from the need to back and restore data this was a painless process. I don&rsquo;t see a noticeable performance impact (though I have a fast system with SSD) the biggest drawback I can see is that if I mess around with a custom kernel and break the boot sequence - I don&rsquo;t know if I can boot from a live CD to fix it.</p>

<p>Setup a restrictive local firewall</p>

<p>as root</p>

<pre><code class="bash ">iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
iptables -I INPUT 1 -i lo -j ACCEPT
iptables -P INPUT DROP
iptables -S
apt-get install iptables-persistent
</code></pre>

<p>edit /etc/postfix/main.cf</p>

<p>set</p>

<p>inet_interfaces = 127.0.0.1</p>

<p>and <code>/etc/init.d/postfix restart</code></p>

<p>I will review open ports each time I configure a new service and ensure I don&rsquo;t have services I only need local accessible externally.</p>

<p>By both configuring firewall <strong>and</strong> limiting services I am applying the principle of defence in depth and even if there is a weakness (or I make a mistake) in one place I will still be protected.</p>

<p>Where I do need to share services between systems on my home/office network I have realised my old router is not sophisticated enough and am purchasing one that can separate secure and insecure networks.</p>

<p>All systems are now being configured for automatic updates, since I want patches as soon as possible, small frequent updates are easier to debug than problems with large updates, and generally I only delay updating out of inertia rather than any deliberate action. This way I don&rsquo;t even have to think about it.</p>

<p>from  <a href="http://www.richud.com/wiki/Ubuntu_Enable_Automatic_Updates_Unattended_Upgrades">http://www.richud.com/wiki/Ubuntu_Enable_Automatic_Updates_Unattended_Upgrades</a></p>

<p>my config files are now</p>

<pre><code class="bash">cat /etc/apt/apt.conf.d/50unattended-upgrades  | grep -v ^//
Unattended-Upgrade::Allowed-Origins {
    "${distro_id}:${distro_codename}-security";
    "${distro_id}:${distro_codename}-updates";
};
Unattended-Upgrade::Package-Blacklist {
};
Unattended-Upgrade::MinimalSteps "true";
Unattended-Upgrade::Mail "root";
Unattended-Upgrade::Remove-Unused-Dependencies "false";
</code></pre>

<pre><code class="bash">cat /etc/apt/apt.conf.d/10periodic  | grep -v ^//
APT::Periodic::Update-Package-Lists "1";
APT::Periodic::Download-Upgradeable-Packages "1";
APT::Periodic::AutocleanInterval "7";
APT::Periodic::Unattended-Upgrade "1";
APT::Periodic::Verbose "2";
APT::Periodic::RandomSleep "1";
</code></pre>

<p>Also set /root/.forward to ensure I get root mail</p>

<p>I have also realised I rely too much on browser stored passwords, and while this is useful for low-security logins I will not be using it for any important site.</p>

<p>Funnily I&rsquo;ve also found I needed to revisit my backup policy and actually delete more stuff <strong>not</strong> backing up code and documents which are business confidential and are already backed up centrally. In this case I realised my own backups were just causing a data management problem just increasing the risk that the data gets accidentally disclosed.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drupageddon]]></title>
    <link href="http://www.practicalweb.co.uk/blog/2014/10/31/drupageddon/"/>
    <updated>2014-10-31T18:30:52+00:00</updated>
    <id>http://www.practicalweb.co.uk/blog/2014/10/31/drupageddon</id>
    <content type="html"><![CDATA[<p>Drupal has been in the new today for all the wrong reasons</p>

<p><a href="http://www.bbc.co.uk/news/technology-29846539">Millions of websites hit by Drupal hack attack</a></p>

<p><a href="http://arstechnica.com/security/2014/10/drupal-sites-had-hours-to-patch-before-attacks-started/">Drupal sites had “hours” to patch before attacks started</a></p>

<p><a href="http://www.pcworld.com/article/2841372/drupal-if-you-werent-quick-to-patch-assume-your-site-was-hacked.html">Drupal users: Assume your site was hacked if you didn&rsquo;t apply Oct. 15 patch immediately</a></p>

<p><a href="http://www.forbes.com/sites/thomasbrewster/2014/10/30/did-drupal-drop-the-ball-users-who-didnt-update-within-7-hours-should-assume-theyve-been-hacked/">Did Drupal Drop The Ball? Users Who Didn&rsquo;t Update Within 7 Hours &lsquo;Should Assume They&rsquo;ve Been Hacked&rsquo;</a></p>

<p>The biggest headache for me was the Public Service Announcement <a href="https://www.drupal.org/PSA-2014-003">https://www.drupal.org/PSA-2014-003</a> which is clearly written to alarm users into updating.</p>

<p>We did update our clients site - within 24 hours which at the time seemed pretty good considering the timezones and chaneg control processes involved.</p>

<p>Fortunately we also reload our entire database and other content on each deploy - so even though we took longer than 7 hours to update we consider we are pretty safe (never say 100%) and beyond that we have a lot of other security in place.</p>

<p>However I find myself wondering what else we could have done.</p>

<p>I wish we had a member on the drupal security team - that seems to have helped some people.</p>

<p>I see now there were a couple of (low key) advance notices <a href="https://groups.drupal.org/node/445893">https://groups.drupal.org/node/445893</a> <a href="https://twitter.com/drupalsecurity/status/522128826101669888">https://twitter.com/drupalsecurity/status/522128826101669888</a></p>

<p>I will try to pay better attention to those in the future, but we may also have to enhance our out of hours work, the security annoucment came in at the end of the day - perhaps we will ahve to work overnight another time to review, test, and help the client upgrade.</p>

<p>Hopefully good will come from the thread at <a href="https://groups.drupal.org/node/447468">Follow up Drupageddon responsibly</a></p>

<p>Drupal will need to learn and grow from this.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hacked Server (Now Restored)]]></title>
    <link href="http://www.practicalweb.co.uk/blog/2013/11/19/hacked-server-now-restored/"/>
    <updated>2013-11-19T00:00:00+00:00</updated>
    <id>http://www.practicalweb.co.uk/blog/2013/11/19/hacked-server-now-restored</id>
    <content type="html"><![CDATA[<p>A few days ago I was notified by my ISP that my server was &quot;emitting a UDP-based denial of service attack &quot;, as a result the VM had been rebooted and taken off the network.</p>




<p>With console access I was able to verify there was a problem, and the ISP was able to give me a clean VM, with my old system available as a mount, in read-only mode</p>




<p>I had thought that the server was fairly well locked down with minimal users, secure passwords and so on, but to be honest with other commitments I hadn&#8217;t been keeping on top of patching.</p>




<p>Reviewing logs etc didn&#8217;t show any clear point of entry, and since the server was rebooted I didn&#8217;t see any  ongoing activity. I decided the best thing is just a clean rebuild, with better security, better monitoring, and keep on top of those patches.</p>




<p>The site is back up - but the code is based on Drupal 5 which isn&#8217;t supported and I don&#8217;t have time for an upgrade right now. What I&#8217;ve done is to run the site  locally (after audit) and put online a static html mirror. As well as leaving less avenue for attack this should make the site a lot faster to load. I keep meaning to revamp the site properly but just never seem to have time, and there are always paid tasks that take priority!</p>




<p>Comments are the main loss - but they were overloaded with spam anyway - so I&#8217;m trying out google plus comments instead which I hope will avoid the spam and give a better experience</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drupal Files Directory Permissions for Continuous Integration]]></title>
    <link href="http://www.practicalweb.co.uk/blog/2011/05/31/drupal-files-directory-permissions-for-continuous-integration/"/>
    <updated>2011-05-31T00:00:00+01:00</updated>
    <id>http://www.practicalweb.co.uk/blog/2011/05/31/drupal-files-directory-permissions-for-continuous-integration</id>
    <content type="html"><![CDATA[<p>I want the drupal user uploaded files to be manageable by both apache and a non-root user</p>

<p>In my case I&rsquo;m using hudson to rebuild the site with data and &ldquo;files&rdquo; from live periodically.</p>

<p>So I have added the apache group to the hudson user</p>

<p><code>
/usr/sbin/usermod -G hudson,apache hudson
/etc/init.d/hudson restart
</code></p>

<p>and set the umask for apache so that files are created group writebale</p>

<p>For Redhat
<code>
echo &ldquo;umask 002&rdquo; >> /etc/sysconfig/httpd
/etc/init.d/httpd restart
</code></p>

<p>For Debian/Ubuntu
<code>
echo &ldquo;umask 002&rdquo; >> /etc/apache2/envvars
/etc/init.d/httpd restart
</code>
NB debian based systems use the group www-data for apache</p>

<p>In my build script I can delete my dev files and copy the ones from live - making sure the group for the files is set to apache.</p>

<p><code>
rm -Rf files
cp -R /tmp/backup/files/ ./
chgrp -R apache files
find files/ -type f -exec chmod 664 &lsquo;{}&rsquo; &lsquo;;&rsquo;
find files/ -type d -exec chmod 775 &lsquo;{}&rsquo; &lsquo;;&rsquo;
</code></p>

<p>Now apache can write the files I have created, and I can create the files apache has created - <b>and</b> they are not world writeable.</p>

<p>see also <a href="http://drupal.org/node/244924">http://drupal.org/node/244924</a></p>

<h2>Update - drush</h2>


<p>I had a cron job running drush as a different user - this created cached files which hudson couldn&rsquo;t delete.</p>

<p>Now I am running the drush cron via hudson and making sure all the &lsquo;files&rsquo; directory stuff is owned correctly afterwards</p>

<p>NB non-root users don&rsquo;t seem to be able to chgrp files that they don&rsquo;t own</p>

<p>so after running cron I make sure any files created by hudson are chgrp&rsquo;d</p>

<p><code>
find files -user hudson -exec chgrp apache {} \;
</code></p>

<p>A simple chgrp -R  caused permission denied errors on those files not owned by hudson - even though they were already group apache so wouldn&rsquo;t have been affected.</p>
]]></content>
  </entry>
  
</feed>
